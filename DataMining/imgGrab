#!/bin/bash

#This script takes one or more URLs as arguments
#It loops through these URLs grabbing the HTML source,
#parsing this source for img tags,
#grabbing the source URL
#and then downloading any images from these URLs.
#These image files are placed in a direcetory named 'images'
#All files smaller than 20k are then deleted from the images directory

for var in "$@"
do
  f=`echo $var | sed 's_.com__g' | sed 's_.org__g' | \
    sed 's_.edu__g' | sed 's_http://__g' | \
    sed 's_www.__g' | sed 's_[/.:]__g'`

  java URLConnectionReader $var | \
  sed 's_img_\n&_g' | grep "^img" | \
  sed 's_src[\s]*=[\s]*"_\n&_g' | \
  grep "^src" | \
  sed 's_src[\s]*=[\s]*"[\s]*__g' | \
  sed 's_\s_\n~_g' | \
  grep "^http" | \
  sed 's_"_\n"_g' | \
  grep "^http" | \
  egrep "jpg$|jpeg$|png$|bmp$" | \
  wget -i - -P ./$f -A jpg,jpeg,png,bmp -q -nc
done

find ./$f -size -20k -exec rm -f {} \;
mv ./$f/*.jpg ./$f/cover.jpg
mv ./$f/*.jpeg ./$f/cover.jpeg
mv ./$f/*.png ./$f/cover.png
mv ./$f/*.bmp ./$f/cover.bmp
